{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP1 : DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(iris.feature_names)\n",
    "print(iris.data)\n",
    "print(iris.target)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP2 : MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C‚Äôest un algorithme d‚Äôapprentissage supervis√© utilis√© en classification (il existe aussi en r√©gression).\n",
    "Il fonctionne en construisant un arbre de d√©cision qui s√©pare progressivement les donn√©es en sous-groupes homog√®nes selon leurs caract√©ristiques (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=200),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'SVC': SVC(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "    'Gaussian Mixture Model (GMM)': GaussianMixture(n_components=3, random_state=42)\n",
    "    \n",
    "}\n",
    "\n",
    "# Split the data (for supervised models only)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP3: TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Accuracy: 1.00\n",
      "LogisticRegression Accuracy: 1.00\n",
      "KNeighborsClassifier Accuracy: 1.00\n",
      "DecisionTreeClassifier Accuracy: 1.00\n",
      "SVC Accuracy: 1.00\n",
      "GaussianNB Accuracy: 0.98\n",
      "GradientBoostingClassifier Accuracy: 1.00\n",
      "LinearDiscriminantAnalysis Accuracy: 1.00\n",
      "KMeans fitted (unsupervised model)\n",
      "Gaussian Mixture Model (GMM) fitted (unsupervised model)\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    if name not in ['KMeans', 'Gaussian Mixture Model (GMM)']:\n",
    "        # Supervised models\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"{name} Accuracy: {acc:.2f}\")\n",
    "    else:\n",
    "        # Unsupervised models\n",
    "        model.fit(X)\n",
    "        print(f\"{name} fitted (unsupervised model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 : TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "['setosa']\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([[5, 2, 4, 3]])\n",
    "print(prediction)\n",
    "print(iris.target_names[prediction])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Mod√®les de Machine Learning sur le jeu de donn√©es Iris\n",
    "\n",
    "## Mod√®les d‚Äôapprentissage supervis√©\n",
    "Les mod√®les supervis√©s apprennent √† partir de **donn√©es √©tiquet√©es** ‚Äî ils connaissent d√©j√† les bonnes r√©ponses pendant l‚Äôentra√Ænement.\n",
    "\n",
    "## 1. DecisionTreeClassifier\n",
    "\n",
    "- Imaginez-le comme un **jeu de questions**.  \n",
    "- Le mod√®le pose des **questions Oui/Non** sur les donn√©es (ex. : ‚ÄúLa longueur des p√©tales > 2 cm ?‚Äù).  \n",
    "- Chaque question divise les donn√©es en petits groupes jusqu‚Äô√† d√©cider √† quelle classe appartient la fleur.  \n",
    "- √Ä la fin, cela ressemble √† un **arbre de d√©cisions**, d‚Äôo√π le nom *Decision Tree*.\n",
    "\n",
    "**En r√©sum√© :**  \n",
    " Il prend des d√©cisions en **posant des questions simples √©tape par √©tape**.\n",
    "\n",
    "\n",
    "##  2. RandomForestClassifier\n",
    "\n",
    "- Une *Random Forest* est juste un **groupe de nombreux arbres de d√©cision** qui travaillent ensemble.  \n",
    "- Chaque arbre donne sa propre d√©cision, et la for√™t prend le **vote majoritaire** pour d√©cider de la r√©ponse finale.  \n",
    "- Comme elle utilise beaucoup d‚Äôarbres, elle est **plus pr√©cise et stable** qu‚Äôun seul arbre.\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "C‚Äôest comme **demander l‚Äôavis de plusieurs petits arbres** et prendre le vote majoritaire.\n",
    "\n",
    "## 3. Support Vector Classifier (SVC)  \n",
    "\n",
    "- Le SVC essaie de **tracer la meilleure ligne (ou fronti√®re)** qui s√©pare les classes.  \n",
    "- Il recherche **l‚Äô√©cart le plus large possible** entre les groupes de points de donn√©es.  \n",
    "- Fonctionne tr√®s bien lorsque les classes sont clairement s√©par√©es, ou m√™me lorsqu‚Äôelles ne le sont pas (en utilisant des transformations sp√©ciales appel√©es *kernels*).\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "Il **trace la fronti√®re la plus claire possible** entre diff√©rents groupes de donn√©es.\n",
    "\n",
    "### R√©gression Logistique vs Classificateur √† Vecteurs de Support (SVC)\n",
    "\n",
    "**Id√©e :** Les deux mod√®les classifient les donn√©es (d√©cident de la classe d‚Äôun √©chantillon), mais ils le font diff√©remment.\n",
    "\n",
    "### üîπ R√©gression Logistique\n",
    "- Regarde **tous les points de donn√©es** et pr√©dit la **probabilit√©** de chaque classe.  \n",
    "- Exemple : ‚ÄúCette fleur a **80% de chances** d‚Äô√™tre Setosa.‚Äù  \n",
    "- Cr√©e une **fronti√®re de d√©cision douce**.\n",
    "\n",
    "### üîπ Classificateur √† Vecteurs de Support (SVC)\n",
    "- Se concentre uniquement sur les **points les plus proches de la fronti√®re**.  \n",
    "- Trace la **s√©paration la plus large possible** entre les classes.  \n",
    "- Exemple : ‚ÄúCette fleur est du c√¥t√© droit ‚Üí c‚Äôest Versicolor.‚Äù  \n",
    "- Cr√©e une **fronti√®re stricte** (√©tiquette de classe, pas de probabilit√©).\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "- R√©gression Logistique = d√©cision bas√©e sur la probabilit√©  \n",
    "- SVC = s√©paration bas√©e sur la marge\n",
    "\n",
    "##  4. GaussianNB (Naive Bayes)  \n",
    "\n",
    "**Id√©e :**  \n",
    "Il regarde la fr√©quence √† laquelle chaque valeur de caract√©ristique appara√Æt dans chaque classe et utilise les **probabilit√©s** pour deviner la classe la plus probable.\n",
    "\n",
    "**Comment √ßa fonctionne (version simple) :**\n",
    "- Il v√©rifie la probabilit√© de chaque valeur pour chaque classe.  \n",
    "- Puis il multiplie ces probabilit√©s (en supposant que chaque caract√©ristique est ind√©pendante).  \n",
    "- Enfin, il choisit la classe avec la **probabilit√© totale la plus √©lev√©e**.\n",
    "\n",
    "**Exemple :**  \n",
    "Imaginez que vous deviez deviner de quelle ville vient quelqu‚Äôun :\n",
    "- Dans la ville A, 80% aiment le th√© et 40% aiment le football.  \n",
    "- Dans la ville B, 30% aiment le th√© et 90% aiment le football.  \n",
    "Si quelqu‚Äôun aime le th√© et le football ‚Üí ville A (0,8√ó0,4=0,32) vs ville B (0,3√ó0,9=0,27).   R√©sultat ‚Üí probablement ville A.\n",
    "\n",
    "**Pourquoi \"Gaussian\" ?**  \n",
    "Parce qu‚Äôil suppose que les nombres (comme la longueur des p√©tales) suivent une **courbe en cloche (distribution normale)**.\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "*Il ‚Äúcompte‚Äù la fr√©quence des √©v√©nements et choisit la classe la plus probable.*\n",
    "\n",
    "##  5. GradientBoostingClassifier\n",
    "\n",
    "- Construit **plusieurs petits arbres de d√©cision**, l‚Äôun apr√®s l‚Äôautre.  \n",
    "- Chaque nouvel arbre essaie de **corriger les erreurs** faites par le pr√©c√©dent.  \n",
    "- Le r√©sultat final est un mod√®le puissant qui combine la connaissance de tous les arbres.\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "Il **apprend √©tape par √©tape**, chaque arbre am√©liorant le pr√©c√©dent.\n",
    "\n",
    "##  6. Logistic Regression\n",
    "\n",
    "**Id√©e :**  \n",
    "Il trace une **ligne fronti√®re** entre les classes et calcule la **probabilit√©** que chaque point appartienne √† une classe.\n",
    "\n",
    "**Comment √ßa fonctionne (version simple) :**\n",
    "- Il combine les caract√©ristiques en une **√©quation lin√©aire**, comme en r√©gression lin√©aire.  \n",
    "- Puis il applique la **fonction logistique (sigmo√Øde)** pour transformer le r√©sultat en **probabilit√© entre 0 et 1**.  \n",
    "- La classe avec la probabilit√© la plus √©lev√©e est choisie.\n",
    "\n",
    "**Exemple :**  \n",
    "S√©parer des points rouges et bleus sur une feuille.  \n",
    "- ‚ÄúC√¥t√© gauche = Rouge‚Äù  \n",
    "- ‚ÄúC√¥t√© droit = Bleu‚Äù\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "*Il utilise une √©quation lin√©aire pour s√©parer les classes et pr√©dit la cat√©gorie √† partir des probabilit√©s.*\n",
    "\n",
    "### 7. KNeighborsClassifier (KNN)    \n",
    "- Examine les **voisins les plus proches** pour d√©cider de la classe d‚Äôun point.  \n",
    "- ‚ÄúMontre-moi tes amis, et je te dirai qui tu es.‚Äù\n",
    "\n",
    "### 8. LinearDiscriminantAnalysis (LDA) \n",
    "\n",
    "LDA est une technique qui nous aide √† **s√©parer les classes plus clairement** en cr√©ant **de nouvelles caract√©ristiques** √† partir des originales.\n",
    "\n",
    "####  Ce qu‚Äôil fait vraiment :\n",
    "- LDA regarde comment chaque classe (Iris-setosa, Iris-versicolor, etc.) est **r√©partie** dans les donn√©es.  \n",
    "- Il cr√©e ensuite de **nouveaux axes** qui rendent les classes **aussi s√©par√©es que possible**.  \n",
    "- Ces nouveaux axes sont **des combinaisons intelligentes des caract√©ristiques originales** (longueur s√©pale, largeur p√©tale, etc.).\n",
    "\n",
    "####  Point important :\n",
    "- LDA **transforme** vos donn√©es ‚Äî les valeurs num√©riques changent ‚Äî mais le **sens reste le m√™me**.\n",
    "\n",
    "####  Exemple :\n",
    "Si vous avez 4 caract√©ristiques dans le dataset Iris, LDA peut les r√©duire √† **1 ou 2 nouvelles caract√©ristiques** tout en gardant le m√™me motif et en rendant la s√©paration entre esp√®ces plus claire.\n",
    "\n",
    " LDA ne d√©truit pas vos donn√©es ‚Äî il les **r√©organise** pour rendre la classification **plus simple et pr√©cise**.\n",
    "\n",
    "##  Mod√®les d‚Äôapprentissage non supervis√©\n",
    "Les mod√®les non supervis√©s apprennent √† partir de **donn√©es non √©tiquet√©es** ‚Äî ils d√©couvrent des structures ou motifs cach√©s.\n",
    "\n",
    "### 9.  KMeans   \n",
    "- Regroupe les donn√©es en **clusters** bas√©s sur leur similarit√©.  \n",
    "- Exemple : trouve 3 groupes de fleurs Iris sans utiliser les labels.\n",
    "\n",
    "### 10.  Gaussian Mixture Model (GMM)\n",
    "\n",
    "**Id√©e :**  \n",
    "C‚Äôest comme une version plus intelligente de **KMeans** pour regrouper les donn√©es sans √©tiquettes.\n",
    "\n",
    "**Comment √ßa marche :**  \n",
    "- Imagine que chaque groupe est un **nuage (une \"gaussienne\")** au lieu d‚Äôun simple cercle. \n",
    "- Chaque point peut appartenir **√† plusieurs groupes en m√™me temps**, avec une **probabilit√©** pour chacun.  \n",
    "- Le mod√®le apprend la **forme et la position** de chaque nuage pour mieux repr√©senter les donn√©es.\n",
    "\n",
    "**Diff√©rence avec KMeans :**  \n",
    "| Mod√®le | Forme des groupes | Attribution |\n",
    "|--------|------------------|-------------|\n",
    "| **KMeans** | Ronds | Chaque point = 1 seul groupe |\n",
    "| **GMM** | Ovaux / flexibles | Chaque point = plusieurs groupes (avec des probabilit√©s) |\n",
    "\n",
    "**En r√©sum√© :**  \n",
    "*Gaussian Mixture regroupe les donn√©es comme KMeans, mais de fa√ßon plus flexible et plus r√©aliste.*\n",
    "\n",
    "##  R√©sum√©\n",
    "\n",
    "| Type | Mod√®le | But |\n",
    "|------|--------|------|\n",
    "| Supervised | Decision Tree | Classer avec des splits |\n",
    "| Supervised | Random Forest | Combiner plusieurs arbres |\n",
    "| Supervised | Logistic Regression | S√©parer les classes avec une ligne |\n",
    "| Supervised | KNN | Utiliser les voisins proches |\n",
    "| Supervised | SVC | Tracer la meilleure fronti√®re |\n",
    "| Supervised | Naive Bayes | Utiliser les probabilit√©s |\n",
    "| Supervised | Gradient Boosting | Apprendre des erreurs |\n",
    "| Supervised | LDA | Trouver la meilleure combinaison de caract√©ristiques |\n",
    "| Unsupervised | KMeans | Regrouper les donn√©es |\n",
    "| Unsupervised | PCA | R√©duire les dimensions |\n",
    "\n",
    "\n",
    "** Sources :** Simplifi√© √† partir de la [documentation Scikit-learn](https://scikit-learn.org/stable/) et  \n",
    "*‚ÄúHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow‚Äù (Aur√©lien G√©ron, O‚ÄôReilly 2023).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NADIRI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
