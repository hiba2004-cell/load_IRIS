{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP1 : DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(iris.feature_names)\n",
    "print(iris.data)\n",
    "print(iris.target)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP2 : MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C’est un algorithme d’apprentissage supervisé utilisé en classification (il existe aussi en régression).\n",
    "Il fonctionne en construisant un arbre de décision qui sépare progressivement les données en sous-groupes homogènes selon leurs caractéristiques (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=200),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'SVC': SVC(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "    'Gaussian Mixture Model (GMM)': GaussianMixture(n_components=3, random_state=42)\n",
    "    \n",
    "}\n",
    "\n",
    "# Split the data (for supervised models only)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP3: TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Accuracy: 1.00\n",
      "LogisticRegression Accuracy: 1.00\n",
      "KNeighborsClassifier Accuracy: 1.00\n",
      "DecisionTreeClassifier Accuracy: 1.00\n",
      "SVC Accuracy: 1.00\n",
      "GaussianNB Accuracy: 0.98\n",
      "GradientBoostingClassifier Accuracy: 1.00\n",
      "LinearDiscriminantAnalysis Accuracy: 1.00\n",
      "KMeans fitted (unsupervised model)\n",
      "Gaussian Mixture Model (GMM) fitted (unsupervised model)\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    if name not in ['KMeans', 'Gaussian Mixture Model (GMM)']:\n",
    "        # Supervised models\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"{name} Accuracy: {acc:.2f}\")\n",
    "    else:\n",
    "        # Unsupervised models\n",
    "        model.fit(X)\n",
    "        print(f\"{name} fitted (unsupervised model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 : TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "['setosa']\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([[5, 2, 4, 3]])\n",
    "print(prediction)\n",
    "print(iris.target_names[prediction])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modèles de Machine Learning sur le jeu de données Iris\n",
    "\n",
    "## Modèles d’apprentissage supervisé\n",
    "Les modèles supervisés apprennent à partir de **données étiquetées** — ils connaissent déjà les bonnes réponses pendant l’entraînement.\n",
    "\n",
    "## 1. DecisionTreeClassifier\n",
    "\n",
    "- Imaginez-le comme un **jeu de questions**.  \n",
    "- Le modèle pose des **questions Oui/Non** sur les données (ex. : “La longueur des pétales > 2 cm ?”).  \n",
    "- Chaque question divise les données en petits groupes jusqu’à décider à quelle classe appartient la fleur.  \n",
    "- À la fin, cela ressemble à un **arbre de décisions**, d’où le nom *Decision Tree*.\n",
    "\n",
    "**En résumé :**  \n",
    " Il prend des décisions en **posant des questions simples étape par étape**.\n",
    "\n",
    "\n",
    "##  2. RandomForestClassifier\n",
    "\n",
    "- Une *Random Forest* est juste un **groupe de nombreux arbres de décision** qui travaillent ensemble.  \n",
    "- Chaque arbre donne sa propre décision, et la forêt prend le **vote majoritaire** pour décider de la réponse finale.  \n",
    "- Comme elle utilise beaucoup d’arbres, elle est **plus précise et stable** qu’un seul arbre.\n",
    "\n",
    "**En résumé :**  \n",
    "C’est comme **demander l’avis de plusieurs petits arbres** et prendre le vote majoritaire.\n",
    "\n",
    "## 3. Support Vector Classifier (SVC)  \n",
    "\n",
    "- Le SVC essaie de **tracer la meilleure ligne (ou frontière)** qui sépare les classes.  \n",
    "- Il recherche **l’écart le plus large possible** entre les groupes de points de données.  \n",
    "- Fonctionne très bien lorsque les classes sont clairement séparées, ou même lorsqu’elles ne le sont pas (en utilisant des transformations spéciales appelées *kernels*).\n",
    "\n",
    "**En résumé :**  \n",
    "Il **trace la frontière la plus claire possible** entre différents groupes de données.\n",
    "\n",
    "### Régression Logistique vs Classificateur à Vecteurs de Support (SVC)\n",
    "\n",
    "**Idée :** Les deux modèles classifient les données (décident de la classe d’un échantillon), mais ils le font différemment.\n",
    "\n",
    "### 🔹 Régression Logistique\n",
    "- Regarde **tous les points de données** et prédit la **probabilité** de chaque classe.  \n",
    "- Exemple : “Cette fleur a **80% de chances** d’être Setosa.”  \n",
    "- Crée une **frontière de décision douce**.\n",
    "\n",
    "### 🔹 Classificateur à Vecteurs de Support (SVC)\n",
    "- Se concentre uniquement sur les **points les plus proches de la frontière**.  \n",
    "- Trace la **séparation la plus large possible** entre les classes.  \n",
    "- Exemple : “Cette fleur est du côté droit → c’est Versicolor.”  \n",
    "- Crée une **frontière stricte** (étiquette de classe, pas de probabilité).\n",
    "\n",
    "**En résumé :**  \n",
    "- Régression Logistique = décision basée sur la probabilité  \n",
    "- SVC = séparation basée sur la marge\n",
    "\n",
    "##  4. GaussianNB (Naive Bayes)  \n",
    "\n",
    "**Idée :**  \n",
    "Il regarde la fréquence à laquelle chaque valeur de caractéristique apparaît dans chaque classe et utilise les **probabilités** pour deviner la classe la plus probable.\n",
    "\n",
    "**Comment ça fonctionne (version simple) :**\n",
    "- Il vérifie la probabilité de chaque valeur pour chaque classe.  \n",
    "- Puis il multiplie ces probabilités (en supposant que chaque caractéristique est indépendante).  \n",
    "- Enfin, il choisit la classe avec la **probabilité totale la plus élevée**.\n",
    "\n",
    "**Exemple :**  \n",
    "Imaginez que vous deviez deviner de quelle ville vient quelqu’un :\n",
    "- Dans la ville A, 80% aiment le thé et 40% aiment le football.  \n",
    "- Dans la ville B, 30% aiment le thé et 90% aiment le football.  \n",
    "Si quelqu’un aime le thé et le football → ville A (0,8×0,4=0,32) vs ville B (0,3×0,9=0,27).   Résultat → probablement ville A.\n",
    "\n",
    "**Pourquoi \"Gaussian\" ?**  \n",
    "Parce qu’il suppose que les nombres (comme la longueur des pétales) suivent une **courbe en cloche (distribution normale)**.\n",
    "\n",
    "**En résumé :**  \n",
    "*Il “compte” la fréquence des événements et choisit la classe la plus probable.*\n",
    "\n",
    "##  5. GradientBoostingClassifier\n",
    "\n",
    "- Construit **plusieurs petits arbres de décision**, l’un après l’autre.  \n",
    "- Chaque nouvel arbre essaie de **corriger les erreurs** faites par le précédent.  \n",
    "- Le résultat final est un modèle puissant qui combine la connaissance de tous les arbres.\n",
    "\n",
    "**En résumé :**  \n",
    "Il **apprend étape par étape**, chaque arbre améliorant le précédent.\n",
    "\n",
    "##  6. Logistic Regression\n",
    "\n",
    "**Idée :**  \n",
    "Il trace une **ligne frontière** entre les classes et calcule la **probabilité** que chaque point appartienne à une classe.\n",
    "\n",
    "**Comment ça fonctionne (version simple) :**\n",
    "- Il combine les caractéristiques en une **équation linéaire**, comme en régression linéaire.  \n",
    "- Puis il applique la **fonction logistique (sigmoïde)** pour transformer le résultat en **probabilité entre 0 et 1**.  \n",
    "- La classe avec la probabilité la plus élevée est choisie.\n",
    "\n",
    "**Exemple :**  \n",
    "Séparer des points rouges et bleus sur une feuille.  \n",
    "- “Côté gauche = Rouge”  \n",
    "- “Côté droit = Bleu”\n",
    "\n",
    "**En résumé :**  \n",
    "*Il utilise une équation linéaire pour séparer les classes et prédit la catégorie à partir des probabilités.*\n",
    "\n",
    "### 7. KNeighborsClassifier (KNN)    \n",
    "- Examine les **voisins les plus proches** pour décider de la classe d’un point.  \n",
    "- “Montre-moi tes amis, et je te dirai qui tu es.”\n",
    "\n",
    "### 8. LinearDiscriminantAnalysis (LDA) \n",
    "\n",
    "LDA est une technique qui nous aide à **séparer les classes plus clairement** en créant **de nouvelles caractéristiques** à partir des originales.\n",
    "\n",
    "####  Ce qu’il fait vraiment :\n",
    "- LDA regarde comment chaque classe (Iris-setosa, Iris-versicolor, etc.) est **répartie** dans les données.  \n",
    "- Il crée ensuite de **nouveaux axes** qui rendent les classes **aussi séparées que possible**.  \n",
    "- Ces nouveaux axes sont **des combinaisons intelligentes des caractéristiques originales** (longueur sépale, largeur pétale, etc.).\n",
    "\n",
    "####  Point important :\n",
    "- LDA **transforme** vos données — les valeurs numériques changent — mais le **sens reste le même**.\n",
    "\n",
    "####  Exemple :\n",
    "Si vous avez 4 caractéristiques dans le dataset Iris, LDA peut les réduire à **1 ou 2 nouvelles caractéristiques** tout en gardant le même motif et en rendant la séparation entre espèces plus claire.\n",
    "\n",
    " LDA ne détruit pas vos données — il les **réorganise** pour rendre la classification **plus simple et précise**.\n",
    "\n",
    "##  Modèles d’apprentissage non supervisé\n",
    "Les modèles non supervisés apprennent à partir de **données non étiquetées** — ils découvrent des structures ou motifs cachés.\n",
    "\n",
    "### 9.  KMeans   \n",
    "- Regroupe les données en **clusters** basés sur leur similarité.  \n",
    "- Exemple : trouve 3 groupes de fleurs Iris sans utiliser les labels.\n",
    "\n",
    "### 10.  Gaussian Mixture Model (GMM)\n",
    "\n",
    "**Idée :**  \n",
    "C’est comme une version plus intelligente de **KMeans** pour regrouper les données sans étiquettes.\n",
    "\n",
    "**Comment ça marche :**  \n",
    "- Imagine que chaque groupe est un **nuage (une \"gaussienne\")** au lieu d’un simple cercle. \n",
    "- Chaque point peut appartenir **à plusieurs groupes en même temps**, avec une **probabilité** pour chacun.  \n",
    "- Le modèle apprend la **forme et la position** de chaque nuage pour mieux représenter les données.\n",
    "\n",
    "**Différence avec KMeans :**  \n",
    "| Modèle | Forme des groupes | Attribution |\n",
    "|--------|------------------|-------------|\n",
    "| **KMeans** | Ronds | Chaque point = 1 seul groupe |\n",
    "| **GMM** | Ovaux / flexibles | Chaque point = plusieurs groupes (avec des probabilités) |\n",
    "\n",
    "**En résumé :**  \n",
    "*Gaussian Mixture regroupe les données comme KMeans, mais de façon plus flexible et plus réaliste.*\n",
    "\n",
    "##  Résumé\n",
    "\n",
    "| Type | Modèle | But |\n",
    "|------|--------|------|\n",
    "| Supervised | Decision Tree | Classer avec des splits |\n",
    "| Supervised | Random Forest | Combiner plusieurs arbres |\n",
    "| Supervised | Logistic Regression | Séparer les classes avec une ligne |\n",
    "| Supervised | KNN | Utiliser les voisins proches |\n",
    "| Supervised | SVC | Tracer la meilleure frontière |\n",
    "| Supervised | Naive Bayes | Utiliser les probabilités |\n",
    "| Supervised | Gradient Boosting | Apprendre des erreurs |\n",
    "| Supervised | LDA | Trouver la meilleure combinaison de caractéristiques |\n",
    "| Unsupervised | KMeans | Regrouper les données |\n",
    "| Unsupervised | PCA | Réduire les dimensions |\n",
    "\n",
    "\n",
    "** Sources :** Simplifié à partir de la [documentation Scikit-learn](https://scikit-learn.org/stable/) et  \n",
    "*“Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow” (Aurélien Géron, O’Reilly 2023).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NADIRI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
